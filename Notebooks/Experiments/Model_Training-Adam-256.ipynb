{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a236e7-d372-4dba-b263-2b9232004c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp39-cp39-manylinux1_x86_64.whl (890.2 MB)\n",
      "\u001b[K     |███████████▊                    | 326.1 MB 115.6 MB/s eta 0:00:05  |█                               | 27.1 MB 8.1 MB/s eta 0:01:48"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |███████████████████████▍        | 649.7 MB 136.3 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 890.2 MB 8.6 kB/s  eta 0:00:011\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp39-cp39-manylinux1_x86_64.whl (24.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.3 MB 80.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in ./.conda/envs/default/lib/python3.9/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 71.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |███████████████▏                | 263.9 MB 133.3 MB/s eta 0:00:03     |████████████▉                   | 222.6 MB 130.0 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 557.1 MB 8.5 kB/s  eta 0:00:011\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |███████████████████████████▉    | 275.7 MB 138.4 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 317.1 MB 30 kB/s \n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 69.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in ./.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in ./.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.1)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 3.5 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.3.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 91.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 52.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/default/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 107.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
      "\u001b[K     |████████████████████████████████| 161 kB 112.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: nvidia-cublas-cu11, urllib3, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, charset-normalizer, certifi, torch, requests, pillow, numpy, torchvision\n",
      "Successfully installed certifi-2022.9.24 charset-normalizer-2.1.1 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pillow-9.3.0 requests-2.28.1 torch-1.13.0 torchvision-0.14.0 urllib3-1.26.12\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395b0240-64c2-4c22-9dba-cdeee092ffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (296 kB)\n",
      "\u001b[K     |████████████████████████████████| 296 kB 108.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 91.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[K     |████████████████████████████████| 965 kB 90.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.0.6 cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b97e282-af72-4e10-af48-407c9c2e5755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.2 MB 8.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in ./.conda/envs/default/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.conda/envs/default/lib/python3.9/site-packages (from pandas) (1.23.5)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "\u001b[K     |████████████████████████████████| 498 kB 104.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in ./.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.5.1 pytz-2022.6\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24b13a2b-3c99-4f19-8c7f-867bdaa14517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-summary in ./.conda/envs/default/lib/python3.9/site-packages (1.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259d544-50b2-43bc-8aa4-e00adde97302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import math\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from torch.optim.optimizer import Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d59fbd-5a92-4c14-b072-58234e3dda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ca7944-16af-4999-b9db-e79bbc70fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    r\"\"\"PyTorch implementation of the lookahead wrapper.\n",
    "\n",
    "    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, la_steps=5, la_alpha=0.8, pullback_momentum=\"none\"):\n",
    "        \"\"\"optimizer: inner optimizer\n",
    "        la_steps (int): number of lookahead steps\n",
    "        la_alpha (float): linear interpolation factor. 1.0 recovers the inner optimizer.\n",
    "        pullback_momentum (str): change to inner optimizer momentum on interpolation update\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self._la_step = 0  # counter for inner optimizer\n",
    "        self.la_alpha = la_alpha\n",
    "        self._total_la_steps = la_steps\n",
    "        pullback_momentum = pullback_momentum.lower()\n",
    "        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n",
    "        self.pullback_momentum = pullback_momentum\n",
    "\n",
    "        self.state = defaultdict(dict)\n",
    "\n",
    "        # Cache the current optimizer parameters\n",
    "        for group in optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['cached_params'] = torch.zeros_like(p.data)\n",
    "                param_state['cached_params'].copy_(p.data)\n",
    "                if self.pullback_momentum == \"pullback\":\n",
    "                    param_state['cached_mom'] = torch.zeros_like(p.data)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            'state': self.state,\n",
    "            'optimizer': self.optimizer,\n",
    "            'la_alpha': self.la_alpha,\n",
    "            '_la_step': self._la_step,\n",
    "            '_total_la_steps': self._total_la_steps,\n",
    "            'pullback_momentum': self.pullback_momentum\n",
    "        }\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def get_la_step(self):\n",
    "        return self._la_step\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "    def _backup_and_load_cache(self):\n",
    "        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n",
    "        \"\"\"\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['backup_params'] = torch.zeros_like(p.data)\n",
    "                param_state['backup_params'].copy_(p.data)\n",
    "                p.data.copy_(param_state['cached_params'])\n",
    "\n",
    "    def _clear_and_load_backup(self):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                p.data.copy_(param_state['backup_params'])\n",
    "                del param_state['backup_params']\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single Lookahead optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = self.optimizer.step(closure)\n",
    "        self._la_step += 1\n",
    "\n",
    "        if self._la_step >= self._total_la_steps:\n",
    "            self._la_step = 0\n",
    "            # Lookahead and cache the current optimizer parameters\n",
    "            for group in self.optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    param_state = self.state[p]\n",
    "                    p.data.mul_(self.la_alpha).add_(param_state['cached_params'], alpha=1.0 - self.la_alpha)  # crucial line\n",
    "                    param_state['cached_params'].copy_(p.data)\n",
    "                    if self.pullback_momentum == \"pullback\":\n",
    "                        internal_momentum = self.optimizer.state[p][\"momentum_buffer\"]\n",
    "                        self.optimizer.state[p][\"momentum_buffer\"] = internal_momentum.mul_(self.la_alpha).add_(\n",
    "                            1.0 - self.la_alpha, param_state[\"cached_mom\"])\n",
    "                        param_state[\"cached_mom\"] = self.optimizer.state[p][\"momentum_buffer\"]\n",
    "                    elif self.pullback_momentum == \"reset\":\n",
    "                        self.optimizer.state[p][\"momentum_buffer\"] = torch.zeros_like(p.data)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55c5d8-12c2-4d2e-b076-c10fea214e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d23d25-bb6d-4db1-bd8f-130bac57cbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71553e89-e79a-4378-bfdb-bf70f6d83bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=kernel_size, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "  \n",
    "  \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        downsample = None\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, pool_size)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f1b4cf-4d79-472e-8408-0f2efe1e5f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            1,728\n",
      "├─BatchNorm2d: 1-2                       128\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─BasicBlock: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  36,864\n",
      "|    |    └─BatchNorm2d: 3-2             128\n",
      "|    |    └─Conv2d: 3-3                  36,864\n",
      "|    |    └─BatchNorm2d: 3-4             128\n",
      "|    |    └─Sequential: 3-5              --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─BasicBlock: 2-2                   --\n",
      "|    |    └─Conv2d: 3-6                  73,728\n",
      "|    |    └─BatchNorm2d: 3-7             256\n",
      "|    |    └─Conv2d: 3-8                  147,456\n",
      "|    |    └─BatchNorm2d: 3-9             256\n",
      "|    |    └─Sequential: 3-10             8,448\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─BasicBlock: 2-3                   --\n",
      "|    |    └─Conv2d: 3-11                 294,912\n",
      "|    |    └─BatchNorm2d: 3-12            512\n",
      "|    |    └─Conv2d: 3-13                 589,824\n",
      "|    |    └─BatchNorm2d: 3-14            512\n",
      "|    |    └─Sequential: 3-15             33,280\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─BasicBlock: 2-4                   --\n",
      "|    |    └─Conv2d: 3-16                 1,179,648\n",
      "|    |    └─BatchNorm2d: 3-17            1,024\n",
      "|    |    └─Conv2d: 3-18                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-19            1,024\n",
      "|    |    └─Sequential: 3-20             132,096\n",
      "├─Linear: 1-7                            5,130\n",
      "=================================================================\n",
      "Total params: 4,903,242\n",
      "Trainable params: 4,903,242\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            1,728\n",
      "├─BatchNorm2d: 1-2                       128\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─BasicBlock: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  36,864\n",
      "|    |    └─BatchNorm2d: 3-2             128\n",
      "|    |    └─Conv2d: 3-3                  36,864\n",
      "|    |    └─BatchNorm2d: 3-4             128\n",
      "|    |    └─Sequential: 3-5              --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─BasicBlock: 2-2                   --\n",
      "|    |    └─Conv2d: 3-6                  73,728\n",
      "|    |    └─BatchNorm2d: 3-7             256\n",
      "|    |    └─Conv2d: 3-8                  147,456\n",
      "|    |    └─BatchNorm2d: 3-9             256\n",
      "|    |    └─Sequential: 3-10             8,448\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─BasicBlock: 2-3                   --\n",
      "|    |    └─Conv2d: 3-11                 294,912\n",
      "|    |    └─BatchNorm2d: 3-12            512\n",
      "|    |    └─Conv2d: 3-13                 589,824\n",
      "|    |    └─BatchNorm2d: 3-14            512\n",
      "|    |    └─Sequential: 3-15             33,280\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─BasicBlock: 2-4                   --\n",
      "|    |    └─Conv2d: 3-16                 1,179,648\n",
      "|    |    └─BatchNorm2d: 3-17            1,024\n",
      "|    |    └─Conv2d: 3-18                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-19            1,024\n",
      "|    |    └─Sequential: 3-20             132,096\n",
      "├─Linear: 1-7                            5,130\n",
      "=================================================================\n",
      "Total params: 4,903,242\n",
      "Trainable params: 4,903,242\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration :\n",
    "\n",
    "layers=[1, 1, 1, 1]\n",
    "kernel_size = 1\n",
    "pool_size = 4\n",
    "model = ResNet(BasicBlock, layers)\n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d2915e-4d45-491a-8c9b-f977d534fb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "Train Loss: 0.671 | Train Acc: 36.584% (18292/50000)\n",
      "Test Loss: 0.608 | Test Acc: 45.490% (4549/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 0.476 | Train Acc: 55.980% (27990/50000)\n",
      "Test Loss: 0.472 | Test Acc: 58.580% (5858/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.384 | Train Acc: 64.840% (32420/50000)\n",
      "Test Loss: 0.494 | Test Acc: 57.700% (5770/10000)\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.327 | Train Acc: 70.222% (35111/50000)\n",
      "Test Loss: 0.400 | Test Acc: 67.090% (6709/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.286 | Train Acc: 74.294% (37147/50000)\n",
      "Test Loss: 0.261 | Test Acc: 77.330% (7733/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.248 | Train Acc: 77.824% (38912/50000)\n",
      "Test Loss: 0.300 | Test Acc: 74.620% (7462/10000)\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.225 | Train Acc: 80.084% (40042/50000)\n",
      "Test Loss: 0.254 | Test Acc: 78.330% (7833/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.204 | Train Acc: 81.796% (40898/50000)\n",
      "Test Loss: 0.273 | Test Acc: 78.910% (7891/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.184 | Train Acc: 83.512% (41756/50000)\n",
      "Test Loss: 0.304 | Test Acc: 75.930% (7593/10000)\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.172 | Train Acc: 84.574% (42287/50000)\n",
      "Test Loss: 0.196 | Test Acc: 83.470% (8347/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 0.158 | Train Acc: 85.856% (42928/50000)\n",
      "Test Loss: 0.190 | Test Acc: 85.070% (8507/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.148 | Train Acc: 86.820% (43410/50000)\n",
      "Test Loss: 0.193 | Test Acc: 83.570% (8357/10000)\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.138 | Train Acc: 87.674% (43837/50000)\n",
      "Test Loss: 0.223 | Test Acc: 81.430% (8143/10000)\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 0.128 | Train Acc: 88.472% (44236/50000)\n",
      "Test Loss: 0.352 | Test Acc: 77.490% (7749/10000)\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 0.124 | Train Acc: 88.976% (44488/50000)\n",
      "Test Loss: 0.146 | Test Acc: 87.950% (8795/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 0.115 | Train Acc: 89.718% (44859/50000)\n",
      "Test Loss: 0.175 | Test Acc: 85.860% (8586/10000)\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 0.108 | Train Acc: 90.436% (45218/50000)\n",
      "Test Loss: 0.140 | Test Acc: 88.790% (8879/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 0.104 | Train Acc: 90.784% (45392/50000)\n",
      "Test Loss: 0.150 | Test Acc: 87.880% (8788/10000)\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 0.097 | Train Acc: 91.248% (45624/50000)\n",
      "Test Loss: 0.163 | Test Acc: 87.310% (8731/10000)\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 0.091 | Train Acc: 91.776% (45888/50000)\n",
      "Test Loss: 0.135 | Test Acc: 89.410% (8941/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 0.087 | Train Acc: 92.136% (46068/50000)\n",
      "Test Loss: 0.139 | Test Acc: 89.260% (8926/10000)\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 0.084 | Train Acc: 92.530% (46265/50000)\n",
      "Test Loss: 0.172 | Test Acc: 87.300% (8730/10000)\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 0.078 | Train Acc: 92.904% (46452/50000)\n",
      "Test Loss: 0.172 | Test Acc: 87.810% (8781/10000)\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 0.073 | Train Acc: 93.332% (46666/50000)\n",
      "Test Loss: 0.212 | Test Acc: 85.860% (8586/10000)\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 0.072 | Train Acc: 93.480% (46740/50000)\n",
      "Test Loss: 0.131 | Test Acc: 90.080% (9008/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 0.069 | Train Acc: 93.766% (46883/50000)\n",
      "Test Loss: 0.140 | Test Acc: 89.860% (8986/10000)\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 0.064 | Train Acc: 94.202% (47101/50000)\n",
      "Test Loss: 0.153 | Test Acc: 89.300% (8930/10000)\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 0.062 | Train Acc: 94.322% (47161/50000)\n",
      "Test Loss: 0.162 | Test Acc: 88.370% (8837/10000)\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 0.057 | Train Acc: 94.912% (47456/50000)\n",
      "Test Loss: 0.167 | Test Acc: 88.550% (8855/10000)\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 0.057 | Train Acc: 94.866% (47433/50000)\n",
      "Test Loss: 0.137 | Test Acc: 90.260% (9026/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 0.056 | Train Acc: 94.916% (47458/50000)\n",
      "Test Loss: 0.140 | Test Acc: 90.410% (9041/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 0.053 | Train Acc: 95.258% (47629/50000)\n",
      "Test Loss: 0.142 | Test Acc: 90.390% (9039/10000)\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 0.051 | Train Acc: 95.444% (47722/50000)\n",
      "Test Loss: 0.153 | Test Acc: 89.700% (8970/10000)\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 0.047 | Train Acc: 95.728% (47864/50000)\n",
      "Test Loss: 0.178 | Test Acc: 88.670% (8867/10000)\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 0.047 | Train Acc: 95.624% (47812/50000)\n",
      "Test Loss: 0.147 | Test Acc: 90.690% (9069/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.044 | Train Acc: 96.018% (48009/50000)\n",
      "Test Loss: 0.137 | Test Acc: 91.140% (9114/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.042 | Train Acc: 96.298% (48149/50000)\n",
      "Test Loss: 0.160 | Test Acc: 90.100% (9010/10000)\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 0.042 | Train Acc: 96.254% (48127/50000)\n",
      "Test Loss: 0.174 | Test Acc: 89.700% (8970/10000)\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.039 | Train Acc: 96.424% (48212/50000)\n",
      "Test Loss: 0.188 | Test Acc: 88.980% (8898/10000)\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.039 | Train Acc: 96.392% (48196/50000)\n",
      "Test Loss: 0.144 | Test Acc: 90.950% (9095/10000)\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.038 | Train Acc: 96.560% (48280/50000)\n",
      "Test Loss: 0.147 | Test Acc: 90.170% (9017/10000)\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.035 | Train Acc: 96.794% (48397/50000)\n",
      "Test Loss: 0.153 | Test Acc: 90.570% (9057/10000)\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.034 | Train Acc: 96.996% (48498/50000)\n",
      "Test Loss: 0.179 | Test Acc: 89.540% (8954/10000)\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.032 | Train Acc: 97.118% (48559/50000)\n",
      "Test Loss: 0.208 | Test Acc: 89.060% (8906/10000)\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.031 | Train Acc: 97.200% (48600/50000)\n",
      "Test Loss: 0.143 | Test Acc: 91.490% (9149/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.030 | Train Acc: 97.252% (48626/50000)\n",
      "Test Loss: 0.168 | Test Acc: 90.760% (9076/10000)\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.030 | Train Acc: 97.350% (48675/50000)\n",
      "Test Loss: 0.157 | Test Acc: 90.750% (9075/10000)\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.029 | Train Acc: 97.290% (48645/50000)\n",
      "Test Loss: 0.194 | Test Acc: 89.230% (8923/10000)\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.030 | Train Acc: 97.270% (48635/50000)\n",
      "Test Loss: 0.180 | Test Acc: 90.130% (9013/10000)\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.029 | Train Acc: 97.484% (48742/50000)\n",
      "Test Loss: 0.156 | Test Acc: 91.150% (9115/10000)\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.027 | Train Acc: 97.650% (48825/50000)\n",
      "Test Loss: 0.156 | Test Acc: 91.190% (9119/10000)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    loss = 100.*train_loss/total\n",
    "    print('Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                     % (loss, acc, correct, total))\n",
    "    model_results[str(epoch)] =  {\"train\" : {\"acc\" : acc,\"loss\" : loss},\"test\" : {}}\n",
    "    \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    loss = 100.*test_loss/total\n",
    "    print('Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                     % (loss, acc, correct, total))\n",
    "    model_results[str(epoch)]['test']  = {\"acc\" : acc,\"loss\" : loss}\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint_adam'):\n",
    "            os.mkdir('checkpoint_adam')\n",
    "        torch.save(state, './checkpoint_adam/ckpt_256.pth')\n",
    "        best_acc = acc\n",
    "        \n",
    "#Model Parameters\n",
    "\n",
    "batch_size = 256\n",
    "lr = 1e-2\n",
    "optim_param = {'la_steps':5,\n",
    "               'la_alpha':0.5\n",
    "              }\n",
    "resume = False \n",
    "model_results = {}\n",
    "\n",
    "#Load model\n",
    "        \n",
    "net = model\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint_adam'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint_adam/ckpt_256.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    Cutout(n_holes=1, length=8)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "base_optim = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "Q = math.floor(len(trainset)/batch_size)\n",
    "optimizer = Lookahead(base_optim, **optim_param)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Q)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+epochs+1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "280a2e82-3826-4946-a1ba-e60a0cf20b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result_adam_256.json', 'w') as fp:\n",
    "    json.dump(model_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6162c83a-02d5-415d-a59d-f7115f0c6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./checkpoint_adam/ckpt_256.pth')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "best_acc = checkpoint['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f88609c-f8ea-4985-a444-b73be5abee9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.49"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3aabef-3499-45f1-ab91-574369acb0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
