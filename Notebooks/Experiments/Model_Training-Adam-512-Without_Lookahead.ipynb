{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a236e7-d372-4dba-b263-2b9232004c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.conda/envs/default/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: torchvision in ./.conda/envs/default/lib/python3.9/site-packages (0.14.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.conda/envs/default/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/envs/default/lib/python3.9/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.conda/envs/default/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in ./.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.1)\n",
      "Requirement already satisfied: wheel in ./.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.conda/envs/default/lib/python3.9/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: numpy in ./.conda/envs/default/lib/python3.9/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in ./.conda/envs/default/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/default/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/default/lib/python3.9/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.conda/envs/default/lib/python3.9/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.conda/envs/default/lib/python3.9/site-packages (from requests->torchvision) (1.26.12)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395b0240-64c2-4c22-9dba-cdeee092ffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./.conda/envs/default/lib/python3.9/site-packages (3.6.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.19 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b97e282-af72-4e10-af48-407c9c2e5755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.conda/envs/default/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.conda/envs/default/lib/python3.9/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/default/lib/python3.9/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.conda/envs/default/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b13a2b-3c99-4f19-8c7f-867bdaa14517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-summary in ./.conda/envs/default/lib/python3.9/site-packages (1.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3259d544-50b2-43bc-8aa4-e00adde97302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import math\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from torch.optim.optimizer import Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d59fbd-5a92-4c14-b072-58234e3dda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    \"\"\"Randomly mask out one or more patches from an image.\n",
    "\n",
    "    Args:\n",
    "        n_holes (int): Number of patches to cut out of each image.\n",
    "        length (int): The length (in pixels) of each square patch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_holes, length):\n",
    "        self.n_holes = n_holes\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (Tensor): Tensor image of size (C, H, W).\n",
    "        Returns:\n",
    "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
    "        \"\"\"\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "\n",
    "        for n in range(self.n_holes):\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1: y2, x1: x2] = 0.\n",
    "\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img = img * mask\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ca7944-16af-4999-b9db-e79bbc70fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    r\"\"\"PyTorch implementation of the lookahead wrapper.\n",
    "\n",
    "    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, la_steps=5, la_alpha=0.8, pullback_momentum=\"none\"):\n",
    "        \"\"\"optimizer: inner optimizer\n",
    "        la_steps (int): number of lookahead steps\n",
    "        la_alpha (float): linear interpolation factor. 1.0 recovers the inner optimizer.\n",
    "        pullback_momentum (str): change to inner optimizer momentum on interpolation update\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self._la_step = 0  # counter for inner optimizer\n",
    "        self.la_alpha = la_alpha\n",
    "        self._total_la_steps = la_steps\n",
    "        pullback_momentum = pullback_momentum.lower()\n",
    "        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n",
    "        self.pullback_momentum = pullback_momentum\n",
    "\n",
    "        self.state = defaultdict(dict)\n",
    "\n",
    "        # Cache the current optimizer parameters\n",
    "        for group in optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['cached_params'] = torch.zeros_like(p.data)\n",
    "                param_state['cached_params'].copy_(p.data)\n",
    "                if self.pullback_momentum == \"pullback\":\n",
    "                    param_state['cached_mom'] = torch.zeros_like(p.data)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            'state': self.state,\n",
    "            'optimizer': self.optimizer,\n",
    "            'la_alpha': self.la_alpha,\n",
    "            '_la_step': self._la_step,\n",
    "            '_total_la_steps': self._total_la_steps,\n",
    "            'pullback_momentum': self.pullback_momentum\n",
    "        }\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def get_la_step(self):\n",
    "        return self._la_step\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "    def _backup_and_load_cache(self):\n",
    "        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n",
    "        \"\"\"\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['backup_params'] = torch.zeros_like(p.data)\n",
    "                param_state['backup_params'].copy_(p.data)\n",
    "                p.data.copy_(param_state['cached_params'])\n",
    "\n",
    "    def _clear_and_load_backup(self):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                p.data.copy_(param_state['backup_params'])\n",
    "                del param_state['backup_params']\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        return self.optimizer.param_groups\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single Lookahead optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = self.optimizer.step(closure)\n",
    "        self._la_step += 1\n",
    "\n",
    "        if self._la_step >= self._total_la_steps:\n",
    "            self._la_step = 0\n",
    "            # Lookahead and cache the current optimizer parameters\n",
    "            for group in self.optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    param_state = self.state[p]\n",
    "                    p.data.mul_(self.la_alpha).add_(param_state['cached_params'], alpha=1.0 - self.la_alpha)  # crucial line\n",
    "                    param_state['cached_params'].copy_(p.data)\n",
    "                    if self.pullback_momentum == \"pullback\":\n",
    "                        internal_momentum = self.optimizer.state[p][\"momentum_buffer\"]\n",
    "                        self.optimizer.state[p][\"momentum_buffer\"] = internal_momentum.mul_(self.la_alpha).add_(\n",
    "                            1.0 - self.la_alpha, param_state[\"cached_mom\"])\n",
    "                        param_state[\"cached_mom\"] = self.optimizer.state[p][\"momentum_buffer\"]\n",
    "                    elif self.pullback_momentum == \"reset\":\n",
    "                        self.optimizer.state[p][\"momentum_buffer\"] = torch.zeros_like(p.data)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d55c5d8-12c2-4d2e-b076-c10fea214e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d23d25-bb6d-4db1-bd8f-130bac57cbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71553e89-e79a-4378-bfdb-bf70f6d83bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,\n",
    "                          kernel_size=kernel_size, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "  \n",
    "  \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        downsample = None\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, pool_size)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12f1b4cf-4d79-472e-8408-0f2efe1e5f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            1,728\n",
      "├─BatchNorm2d: 1-2                       128\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─BasicBlock: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  36,864\n",
      "|    |    └─BatchNorm2d: 3-2             128\n",
      "|    |    └─Conv2d: 3-3                  36,864\n",
      "|    |    └─BatchNorm2d: 3-4             128\n",
      "|    |    └─Sequential: 3-5              --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─BasicBlock: 2-2                   --\n",
      "|    |    └─Conv2d: 3-6                  73,728\n",
      "|    |    └─BatchNorm2d: 3-7             256\n",
      "|    |    └─Conv2d: 3-8                  147,456\n",
      "|    |    └─BatchNorm2d: 3-9             256\n",
      "|    |    └─Sequential: 3-10             8,448\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─BasicBlock: 2-3                   --\n",
      "|    |    └─Conv2d: 3-11                 294,912\n",
      "|    |    └─BatchNorm2d: 3-12            512\n",
      "|    |    └─Conv2d: 3-13                 589,824\n",
      "|    |    └─BatchNorm2d: 3-14            512\n",
      "|    |    └─Sequential: 3-15             33,280\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─BasicBlock: 2-4                   --\n",
      "|    |    └─Conv2d: 3-16                 1,179,648\n",
      "|    |    └─BatchNorm2d: 3-17            1,024\n",
      "|    |    └─Conv2d: 3-18                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-19            1,024\n",
      "|    |    └─Sequential: 3-20             132,096\n",
      "├─Linear: 1-7                            5,130\n",
      "=================================================================\n",
      "Total params: 4,903,242\n",
      "Trainable params: 4,903,242\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            1,728\n",
      "├─BatchNorm2d: 1-2                       128\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─BasicBlock: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  36,864\n",
      "|    |    └─BatchNorm2d: 3-2             128\n",
      "|    |    └─Conv2d: 3-3                  36,864\n",
      "|    |    └─BatchNorm2d: 3-4             128\n",
      "|    |    └─Sequential: 3-5              --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─BasicBlock: 2-2                   --\n",
      "|    |    └─Conv2d: 3-6                  73,728\n",
      "|    |    └─BatchNorm2d: 3-7             256\n",
      "|    |    └─Conv2d: 3-8                  147,456\n",
      "|    |    └─BatchNorm2d: 3-9             256\n",
      "|    |    └─Sequential: 3-10             8,448\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─BasicBlock: 2-3                   --\n",
      "|    |    └─Conv2d: 3-11                 294,912\n",
      "|    |    └─BatchNorm2d: 3-12            512\n",
      "|    |    └─Conv2d: 3-13                 589,824\n",
      "|    |    └─BatchNorm2d: 3-14            512\n",
      "|    |    └─Sequential: 3-15             33,280\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─BasicBlock: 2-4                   --\n",
      "|    |    └─Conv2d: 3-16                 1,179,648\n",
      "|    |    └─BatchNorm2d: 3-17            1,024\n",
      "|    |    └─Conv2d: 3-18                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-19            1,024\n",
      "|    |    └─Sequential: 3-20             132,096\n",
      "├─Linear: 1-7                            5,130\n",
      "=================================================================\n",
      "Total params: 4,903,242\n",
      "Trainable params: 4,903,242\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration :\n",
    "\n",
    "layers=[1, 1, 1, 1]\n",
    "kernel_size = 1\n",
    "pool_size = 4\n",
    "model = ResNet(BasicBlock, layers)\n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d2915e-4d45-491a-8c9b-f977d534fb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "Train Loss: 0.368 | Train Acc: 30.582% (15291/50000)\n",
      "Test Loss: 0.328 | Test Acc: 39.920% (3992/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "Train Loss: 0.286 | Train Acc: 46.046% (23023/50000)\n",
      "Test Loss: 0.271 | Test Acc: 51.380% (5138/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "Train Loss: 0.240 | Train Acc: 55.652% (27826/50000)\n",
      "Test Loss: 0.236 | Test Acc: 58.840% (5884/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "Train Loss: 0.204 | Train Acc: 62.718% (31359/50000)\n",
      "Test Loss: 0.205 | Test Acc: 65.340% (6534/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "Train Loss: 0.177 | Train Acc: 67.860% (33930/50000)\n",
      "Test Loss: 0.171 | Test Acc: 70.160% (7016/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      "Train Loss: 0.160 | Train Acc: 71.250% (35625/50000)\n",
      "Test Loss: 0.166 | Test Acc: 71.240% (7124/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      "Train Loss: 0.144 | Train Acc: 73.986% (36993/50000)\n",
      "Test Loss: 0.152 | Test Acc: 74.060% (7406/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 7\n",
      "Train Loss: 0.129 | Train Acc: 76.896% (38448/50000)\n",
      "Test Loss: 0.143 | Test Acc: 75.760% (7576/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      "Train Loss: 0.117 | Train Acc: 79.158% (39579/50000)\n",
      "Test Loss: 0.131 | Test Acc: 78.320% (7832/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      "Train Loss: 0.106 | Train Acc: 80.862% (40431/50000)\n",
      "Test Loss: 0.121 | Test Acc: 79.670% (7967/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      "Train Loss: 0.098 | Train Acc: 82.588% (41294/50000)\n",
      "Test Loss: 0.126 | Test Acc: 79.330% (7933/10000)\n",
      "\n",
      "Epoch: 11\n",
      "Train Loss: 0.089 | Train Acc: 84.284% (42142/50000)\n",
      "Test Loss: 0.109 | Test Acc: 81.810% (8181/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      "Train Loss: 0.083 | Train Acc: 85.162% (42581/50000)\n",
      "Test Loss: 0.110 | Test Acc: 81.780% (8178/10000)\n",
      "\n",
      "Epoch: 13\n",
      "Train Loss: 0.078 | Train Acc: 86.054% (43027/50000)\n",
      "Test Loss: 0.123 | Test Acc: 80.370% (8037/10000)\n",
      "\n",
      "Epoch: 14\n",
      "Train Loss: 0.072 | Train Acc: 87.236% (43618/50000)\n",
      "Test Loss: 0.099 | Test Acc: 84.320% (8432/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      "Train Loss: 0.068 | Train Acc: 87.840% (43920/50000)\n",
      "Test Loss: 0.087 | Test Acc: 85.820% (8582/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 16\n",
      "Train Loss: 0.063 | Train Acc: 88.686% (44343/50000)\n",
      "Test Loss: 0.085 | Test Acc: 86.350% (8635/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 17\n",
      "Train Loss: 0.059 | Train Acc: 89.342% (44671/50000)\n",
      "Test Loss: 0.087 | Test Acc: 86.200% (8620/10000)\n",
      "\n",
      "Epoch: 18\n",
      "Train Loss: 0.056 | Train Acc: 89.974% (44987/50000)\n",
      "Test Loss: 0.082 | Test Acc: 86.860% (8686/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 19\n",
      "Train Loss: 0.053 | Train Acc: 90.494% (45247/50000)\n",
      "Test Loss: 0.084 | Test Acc: 87.170% (8717/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 20\n",
      "Train Loss: 0.049 | Train Acc: 91.170% (45585/50000)\n",
      "Test Loss: 0.086 | Test Acc: 87.000% (8700/10000)\n",
      "\n",
      "Epoch: 21\n",
      "Train Loss: 0.047 | Train Acc: 91.540% (45770/50000)\n",
      "Test Loss: 0.083 | Test Acc: 87.620% (8762/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 22\n",
      "Train Loss: 0.045 | Train Acc: 91.814% (45907/50000)\n",
      "Test Loss: 0.078 | Test Acc: 87.790% (8779/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 23\n",
      "Train Loss: 0.042 | Train Acc: 92.354% (46177/50000)\n",
      "Test Loss: 0.086 | Test Acc: 87.220% (8722/10000)\n",
      "\n",
      "Epoch: 24\n",
      "Train Loss: 0.039 | Train Acc: 93.024% (46512/50000)\n",
      "Test Loss: 0.094 | Test Acc: 86.680% (8668/10000)\n",
      "\n",
      "Epoch: 25\n",
      "Train Loss: 0.038 | Train Acc: 93.092% (46546/50000)\n",
      "Test Loss: 0.078 | Test Acc: 88.530% (8853/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 26\n",
      "Train Loss: 0.035 | Train Acc: 93.658% (46829/50000)\n",
      "Test Loss: 0.088 | Test Acc: 87.470% (8747/10000)\n",
      "\n",
      "Epoch: 27\n",
      "Train Loss: 0.034 | Train Acc: 93.870% (46935/50000)\n",
      "Test Loss: 0.081 | Test Acc: 88.500% (8850/10000)\n",
      "\n",
      "Epoch: 28\n",
      "Train Loss: 0.031 | Train Acc: 94.514% (47257/50000)\n",
      "Test Loss: 0.098 | Test Acc: 87.100% (8710/10000)\n",
      "\n",
      "Epoch: 29\n",
      "Train Loss: 0.031 | Train Acc: 94.526% (47263/50000)\n",
      "Test Loss: 0.087 | Test Acc: 88.500% (8850/10000)\n",
      "\n",
      "Epoch: 30\n",
      "Train Loss: 0.028 | Train Acc: 95.042% (47521/50000)\n",
      "Test Loss: 0.081 | Test Acc: 89.260% (8926/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 31\n",
      "Train Loss: 0.027 | Train Acc: 95.046% (47523/50000)\n",
      "Test Loss: 0.088 | Test Acc: 88.590% (8859/10000)\n",
      "\n",
      "Epoch: 32\n",
      "Train Loss: 0.025 | Train Acc: 95.604% (47802/50000)\n",
      "Test Loss: 0.083 | Test Acc: 89.230% (8923/10000)\n",
      "\n",
      "Epoch: 33\n",
      "Train Loss: 0.023 | Train Acc: 95.974% (47987/50000)\n",
      "Test Loss: 0.079 | Test Acc: 89.860% (8986/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 34\n",
      "Train Loss: 0.023 | Train Acc: 95.818% (47909/50000)\n",
      "Test Loss: 0.082 | Test Acc: 89.330% (8933/10000)\n",
      "\n",
      "Epoch: 35\n",
      "Train Loss: 0.021 | Train Acc: 96.254% (48127/50000)\n",
      "Test Loss: 0.088 | Test Acc: 89.130% (8913/10000)\n",
      "\n",
      "Epoch: 36\n",
      "Train Loss: 0.021 | Train Acc: 96.326% (48163/50000)\n",
      "Test Loss: 0.081 | Test Acc: 89.710% (8971/10000)\n",
      "\n",
      "Epoch: 37\n",
      "Train Loss: 0.018 | Train Acc: 96.722% (48361/50000)\n",
      "Test Loss: 0.081 | Test Acc: 89.740% (8974/10000)\n",
      "\n",
      "Epoch: 38\n",
      "Train Loss: 0.017 | Train Acc: 96.906% (48453/50000)\n",
      "Test Loss: 0.088 | Test Acc: 89.560% (8956/10000)\n",
      "\n",
      "Epoch: 39\n",
      "Train Loss: 0.018 | Train Acc: 96.840% (48420/50000)\n",
      "Test Loss: 0.079 | Test Acc: 90.300% (9030/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 40\n",
      "Train Loss: 0.015 | Train Acc: 97.210% (48605/50000)\n",
      "Test Loss: 0.085 | Test Acc: 89.790% (8979/10000)\n",
      "\n",
      "Epoch: 41\n",
      "Train Loss: 0.015 | Train Acc: 97.444% (48722/50000)\n",
      "Test Loss: 0.090 | Test Acc: 89.510% (8951/10000)\n",
      "\n",
      "Epoch: 42\n",
      "Train Loss: 0.015 | Train Acc: 97.262% (48631/50000)\n",
      "Test Loss: 0.088 | Test Acc: 89.700% (8970/10000)\n",
      "\n",
      "Epoch: 43\n",
      "Train Loss: 0.013 | Train Acc: 97.622% (48811/50000)\n",
      "Test Loss: 0.090 | Test Acc: 89.630% (8963/10000)\n",
      "\n",
      "Epoch: 44\n",
      "Train Loss: 0.013 | Train Acc: 97.712% (48856/50000)\n",
      "Test Loss: 0.085 | Test Acc: 90.600% (9060/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 45\n",
      "Train Loss: 0.013 | Train Acc: 97.730% (48865/50000)\n",
      "Test Loss: 0.084 | Test Acc: 90.500% (9050/10000)\n",
      "\n",
      "Epoch: 46\n",
      "Train Loss: 0.011 | Train Acc: 97.986% (48993/50000)\n",
      "Test Loss: 0.085 | Test Acc: 90.370% (9037/10000)\n",
      "\n",
      "Epoch: 47\n",
      "Train Loss: 0.012 | Train Acc: 97.948% (48974/50000)\n",
      "Test Loss: 0.086 | Test Acc: 90.430% (9043/10000)\n",
      "\n",
      "Epoch: 48\n",
      "Train Loss: 0.010 | Train Acc: 98.192% (49096/50000)\n",
      "Test Loss: 0.087 | Test Acc: 90.350% (9035/10000)\n",
      "\n",
      "Epoch: 49\n",
      "Train Loss: 0.010 | Train Acc: 98.238% (49119/50000)\n",
      "Test Loss: 0.091 | Test Acc: 90.000% (9000/10000)\n",
      "\n",
      "Epoch: 50\n",
      "Train Loss: 0.010 | Train Acc: 98.280% (49140/50000)\n",
      "Test Loss: 0.085 | Test Acc: 90.780% (9078/10000)\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100.*correct/total\n",
    "    loss = 100.*train_loss/total\n",
    "    print('Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'\n",
    "                     % (loss, acc, correct, total))\n",
    "    model_results[str(epoch)] =  {\"train\" : {\"acc\" : acc,\"loss\" : loss},\"test\" : {}}\n",
    "    \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    loss = 100.*test_loss/total\n",
    "    print('Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
    "                     % (loss, acc, correct, total))\n",
    "    model_results[str(epoch)]['test']  = {\"acc\" : acc,\"loss\" : loss}\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint_adam'):\n",
    "            os.mkdir('checkpoint_adam')\n",
    "        torch.save(state, './checkpoint_adam/ckpt_512_without_lookahead.pth')\n",
    "        best_acc = acc\n",
    "        \n",
    "#Model Parameters\n",
    "\n",
    "batch_size = 512\n",
    "lr = 1e-2\n",
    "optim_param = {'la_steps':5,\n",
    "               'la_alpha':0.5\n",
    "              }\n",
    "resume = False \n",
    "model_results = {}\n",
    "\n",
    "#Load model\n",
    "        \n",
    "net = model\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint_adam'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint_adam/ckpt_512_without_lookahead.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    Cutout(n_holes=1, length=8)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "Q = math.floor(len(trainset)/batch_size)\n",
    "# optimizer = Lookahead(base_optim, **optim_param)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Q)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+epochs+1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "280a2e82-3826-4946-a1ba-e60a0cf20b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result_adam_512_without_lookahead.json', 'w') as fp:\n",
    "    json.dump(model_results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6162c83a-02d5-415d-a59d-f7115f0c6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./checkpoint_adam/ckpt_512_without_lookahead.pth')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "best_acc = checkpoint['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88609c-f8ea-4985-a444-b73be5abee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29ee6c-eff0-4fa6-a8e8-c0a5117fff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results[\"1\"][\"train\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd713f-c930-48df-b663-79e8121a4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3aabef-3499-45f1-ab91-574369acb0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
